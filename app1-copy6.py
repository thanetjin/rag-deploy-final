from langchain_core.messages import HumanMessage, AIMessage,SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone
from dotenv import load_dotenv
import io
import re
import streamlit as st
import os
import tempfile
import time
from PyPDF2 import PdfReader
from langchain.schema import Document

os.environ["LLAMA_CLOUD_API_KEY"] = st.secrets.get("LLAMA_CLOUD_API_KEY")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = st.secrets.get("HUGGINGFACEHUB_API_TOKEN")
import nest_asyncio
nest_asyncio.apply()

load_dotenv()

# Load API Keys

pinecone_api_key = st.secrets.get("pinecone_api_key")
# Initialize Pinecone
pc = Pinecone(api_key=pinecone_api_key)
index_list = pc.list_indexes()
index_names = [index_info["name"] for index_info in index_list.get("indexes", [])]
print("Index names are:", index_names[0])

index = pc.Index(index_names[0])
huggingface_ef = HuggingFaceInferenceAPIEmbeddings(
    api_key=os.getenv("HUGGINGFACEHUB_API_TOKEN"),
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
)

vector_store = PineconeVectorStore(index=index, embedding=huggingface_ef)

def handle_upload():
    # This is triggered when a file is uploaded
    st.session_state.file_uploaded = True

if "chat_history" not in st.session_state:
    st.session_state.chat_history = [
        AIMessage(content="‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö‡∏ú‡∏°‡∏Ñ‡∏£‡∏π‡πÄ‡∏Å‡πà‡∏á üë®üèª‚Äçüíª ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏´‡∏•‡∏±‡∏Å‡∏™‡∏π‡∏ï‡∏£‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≠‡∏°‡∏û‡∏¥‡∏ß‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÑ‡∏î‡πâ‡πÉ‡∏ô‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ô‡∏∞‡∏Ñ‡∏£‡∏±‡∏ö")
    ]
if "last_interaction" not in st.session_state:
    st.session_state.last_interaction = time.time()
    # Global flag to track deletio
if "deletion_scheduled" not in st.session_state:
    st.session_state.deletion_scheduled = False
if "student_id" not in st.session_state:    
    st.session_state["student_id"] = None  # or some default value
if "extracted_text" not in st.session_state:
    st.session_state.extracted_text = None
if "is_transcript_uploaded" not in st.session_state:
    st.session_state.is_transcript_uploaded = False  # Controls if a valid transcript was uploaded
if "file_uploaded" not in st.session_state:
    st.session_state.file_uploaded = False  # False means user has not uploaded yet
if "temp_transcript_data" not in st.session_state:
    st.session_state.temp_transcript_data = None
st.header("üí¨ Chat with Kru Keng")
st.title("üë®‚Äçüè´ Ask me anything about the Computer Science curriculum at Kasetsart University or academic policies and procedures")
uploaded_file = st.file_uploader(
    "Upload your transcript", 
    type=["pdf"], 
    disabled=st.session_state.is_transcript_uploaded,  # Disable only if a transcript is confirmed
    on_change=handle_upload
)

def get_retriever_chain():
    """Retrieve documents based on user queries, distinguishing between course and transcript data."""
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")         
    metadata_filter = {
        "$or": [            
            {"type": "course"},
            {"type": "english"},
            {"type": "ged-ed"},
            {"type": "policy"}
        ]
    }
    retriever = vector_store.as_retriever(search_kwargs={'k': 100, 'filter': metadata_filter})

    prompt = ChatPromptTemplate.from_messages([
    MessagesPlaceholder(variable_name="chat_history"),
    ("user", "{input}"),
    (
        "user",
        "‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ: '‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤', "
        "'‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤', ‡∏´‡∏£‡∏∑‡∏≠ '‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á' ‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡πâ‡∏ô‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\n\n"
        "**‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:**\n"
        "- ‡∏´‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤ ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏ó‡∏µ‡πà‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ñ‡∏π‡∏Å‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏ã‡πâ‡∏≥\n"
        "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏Å‡πà‡∏≠‡∏ô (‡∏´‡∏≤‡∏Å‡∏°‡∏µ) ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÉ‡∏´‡∏°‡πà\n"
        "- ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô"
    )
])
    history_retriever_chain = create_history_aware_retriever(llm, retriever, prompt)
    return history_retriever_chain

def get_conversational_rag(history_retriever_chain):
    """Creates a RAG chain that dynamically retrieves course and transcript information."""
    llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash")

    answer_prompt = ChatPromptTemplate.from_messages([
        ("system", """
        ‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢ AI ‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏•‡∏á‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡πÅ‡∏•‡∏∞‡∏ô‡πÇ‡∏¢‡∏ö‡∏≤‡∏¢‡∏ó‡∏≤‡∏á‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
        ‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô‡πÅ‡∏•‡∏∞‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÇ‡∏î‡∏¢‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á
        ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô

        **‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°:**
        - ‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô
        - ‡∏´‡∏≤‡∏Å‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÑ‡∏°‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô ‡πÉ‡∏´‡πâ‡∏Ç‡∏≠‡∏£‡∏≤‡∏¢‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏à‡∏≤‡∏Å‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤
        - ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡πÉ‡∏´‡πâ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ô‡∏±‡∏Å‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏∞‡πÄ‡∏ö‡∏µ‡∏¢‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ù‡πà‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡∏Å‡∏≤‡∏£
        - ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î transcirpt (‡πÉ‡∏ö‡πÄ‡∏Å‡∏£‡∏î) ‡∏à‡∏á‡πÉ‡∏ä‡πâ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏≠‡∏±‡∏û‡πÇ‡∏´‡∏•‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏´‡∏≤‡∏Å‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö transcript ‡∏ô‡∏±‡∏Å‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡∏≤‡∏¢‡∏ß‡∏¥‡∏ä‡∏≤‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏™‡∏≠‡∏ö‡∏ñ‡∏≤‡∏°
        - ‡∏´‡∏≤‡∏Å‡πÉ‡∏ô knowledgebase ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏∑‡∏ö‡∏Ñ‡πâ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏°‡∏´‡∏≤‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡∏•‡∏±‡∏¢‡πÄ‡∏Å‡∏©‡∏ï‡∏£‡∏®‡∏≤‡∏ï‡∏£‡πå ‡∏õ‡∏£‡∏∞‡πÄ‡∏ó‡∏®‡πÑ‡∏ó‡∏¢ ‡πÑ‡∏î‡πâ 

        **‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:**
        {context}
        """),
        MessagesPlaceholder(variable_name="chat_history"),
        ("user", "{input}")
    ])

    document_chain = create_stuff_documents_chain(llm, answer_prompt)
    conversational_retrieval_chain = create_retrieval_chain(history_retriever_chain, document_chain)
    return conversational_retrieval_chain

# Returns the final response
def get_response(user_input, extracted_text):
    """This function uses‡∏ï the extracted text from the PDF directly in the prompt."""
    history_retriever_chain = get_retriever_chain()
    conversation_rag_chain = get_conversational_rag(history_retriever_chain)

    response = conversation_rag_chain.invoke({
        "chat_history": st.session_state.chat_history,
        "input": user_input,        
    })

    return response["answer"]

# Function to extract Student ID dynamically
def extract_student_id(text):
    match = re.search(r"Student\s*No[:\s]*([\d]+)", text, re.IGNORECASE)  
    return match.group(1) if match else None

# Add this to your session state initialization
if "temp_transcript_data" not in st.session_state:
    st.session_state.temp_transcript_data = None

if uploaded_file and st.session_state.file_uploaded:  
    print("uploaded_file is : ",uploaded_file)
    print("st.session_state.file_uploaded : ",st.session_state.file_uploaded)
    try:
        with st.spinner("Processing PDF..."):
            # Save the uploaded file temporarily
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_file:
                temp_file.write(uploaded_file.getbuffer())
                temp_file_path = temp_file.name

            # Extract text from PDF
            extracted_text = ""
            with open(temp_file_path, "rb") as file:
                reader = PdfReader(file)
                for page in reader.pages:
                    extracted_text += page.extract_text() + "\n"            
            is_transcript = "Date Of Graduation" in extracted_text
            
            if is_transcript:
                st.success("The uploaded document is identified as a transcript.")                
                student_id = extract_student_id(extracted_text)
                st.session_state["student_id"] = student_id  # Store it for later use            
                st.session_state["extracted_text"] = extracted_text  # Store the extracted text                                    

                # ‚úÖ Store the transcript message but disable further uploads
                message = HumanMessage(content=extracted_text)  
                st.session_state.chat_history.append(message)                
                st.session_state.is_transcript_uploaded = True  # ‚úÖ Prevent further uploads
            else:                
                st.error("‚ö†Ô∏è The uploaded document does not appear to be a transcript. Please re-upload.")
                st.session_state.file_uploaded = False  # ‚úÖ Allow re-uploading

    except Exception as e:
        st.error(f"An error occurred: {str(e)}")
    
    finally:
        # Clean up temporary files
        if os.path.exists(temp_file_path):
            os.remove(temp_file_path)
user_input = st.chat_input("Type your message here...")
if user_input and user_input.strip():
    # Use the extracted text from session state if available
    transcript_text = st.session_state.get("extracted_text")
    if transcript_text:
        print("extracted_text ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à:", transcript_text[:100])  # Print first 100 chars
    else:
        print("No extracted text found in session state")
    response = get_response(user_input, extracted_text=transcript_text)
    st.session_state.chat_history.append(HumanMessage(content=user_input))
    st.session_state.chat_history.append(AIMessage(content=response))
    st.session_state.last_interaction = time.time()

# ‚úÖ Display chat history but SKIP the transcript upload message
for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage) and message.content == st.session_state.get("extracted_text"):
        continue  # Skip displaying the transcript message
    with st.chat_message("AI" if isinstance(message, AIMessage) else "Human"):
        st.write(message.content)
